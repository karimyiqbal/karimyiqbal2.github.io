[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "Research Paper"
  },
  {
    "objectID": "research.html#research-paper",
    "href": "research.html#research-paper",
    "title": "Research",
    "section": "",
    "text": "Research Paper"
  },
  {
    "objectID": "research.html#research-proposal",
    "href": "research.html#research-proposal",
    "title": "Research",
    "section": "Research Proposal",
    "text": "Research Proposal\nResearch Proposal"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I am currently pursuing masters from the University of Texas at Dallas."
  },
  {
    "objectID": "about.html#about",
    "href": "about.html#about",
    "title": "About",
    "section": "",
    "text": "I am currently pursuing masters from the University of Texas at Dallas."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Home",
    "section": "",
    "text": "Even after doing things I end up paying for something"
  },
  {
    "objectID": "index.html#karim-iqbal",
    "href": "index.html#karim-iqbal",
    "title": "Home",
    "section": "",
    "text": "Even after doing things I end up paying for something"
  },
  {
    "objectID": "index.html#epps-6302---assignments",
    "href": "index.html#epps-6302---assignments",
    "title": "Home",
    "section": "EPPS 6302 - Assignments",
    "text": "EPPS 6302 - Assignments\nAssignment 1\n2a. The questionnaire primarily employs Multiple Choice (MC) questions, supplemented by a Descriptive Block (DB) and a Matrix question. The DB serves as an introductory segment, providing context but not soliciting data. The MC questions, forming the survey’s core, offer predefined choices for concise response gathering. An example includes querying preferences between purchasing or renting movies. The Matrix question, distinct in its format, allows for the simultaneous assessment of various service aspects, streamlining comparative analysis within a singular framework. This combination of question types facilitates an efficient yet comprehensive data collection approach.\n2b. The questionnaire of this survey strikes a balance in its composition, primarily utilizing two types of questions. It includes descriptive text blocks, which serve to provide essential instructions or contextual information to the respondents, effectively setting the stage for the survey. Alongside these, the survey incorporates multiple-choice (MC) questions. These questions ask respondents to select from a set of predefined options, offering a clear and structured way to gather data. This combination of descriptive guidance and specific, choice-based questions ensures both clarity and ease of participation, crucial for effective data gathering in a survey context.\n2c. The questions are ordered logically, starting with a descriptive text block that likely serves as an introduction or instructions to the respondents. This is followed by multiple-choice questions covering different aspects of movie rental experiences. The questions seem to be grouped by theme, progressing in a way that maintains a logical flow and coherence in the survey.\nThis analysis is based on the initial elements and the first few questions. For a more detailed understanding, a deeper analysis of each question and its specific content would be necessary.\n7. To enhance the respondent experience, a few adjustments could be beneficial. First, considering the survey has 18 questions, ensuring relevance of each question is key to maintain respondent engagement. Additionally, including a few open-ended questions could provide richer, qualitative insights, as the current survey lacks ‘Text Entry’ (TE) type questions. Finally, for questions with more than five options, simplifying choices or using a drop-down format could help prevent respondent fatigue and make the survey more user-friendly.\nFurther assignment\n4. The question delves into demographics beyond just race and ethnicity, probing into aspects like socioeconomic status and highest level of education. This approach enables a more comprehensive understanding of diversity and inclusion. By gaining insights into these deeper demographic facets, it becomes possible to draw nuanced conclusions about the respondents’ backgrounds and how these relate to their perspectives on diversity and inclusiveness.\n\nAssignment 2\nUse Google Trends website to: Search Trump, Biden and Election\nDownload the data Analyze the data\nDates\nIn the period analyzed from 10/10/2018 to 10/10/2023, the most searches for Trump, Biden, and election happened during the first week of November 2020, which is right before the presidential election. Two other spikes are seen around Jan 6th events and the mid-term elections in November 2022.\nIntervals\nThe intervals between peaks are irregular, but the most notable peaks align with important political events, such as elections. The data shows seasonal trends with higher search interest around the time of elections and significant political events.\n\nUse gtrendsR package to do a. (use gtrendsR01.R program)\n\nWhat are the differences between the two methods?\nGoogle Trends provides an intuitive web interface that allows users to easily visualize search interest over time and download data, but for deeper analysis or data manipulation, one must use external tools. In contrast, gtrendsR allows for direct data manipulation and complex analyses, leveraging R’s statistical capabilities. While Google Trends offers a simple and direct way to look at trends, gtrendsR provides more flexibility in data presentation, enabling users to customize plots extensively and integrate Google Trends data with other datasets for comprehensive analysis.\nAssignment 3\nText Analytics using quanteda\n1.Read about the package quanteda at https://quanteda.io/\n2.Download quanteda_textanalytics01.R from Teams\n3.Analyze:\na. Biden-Xi summit data\nQuanteda package tokenized the tweets, created a document-feature matrix, and applied Latent Semantic Analysis to uncover underlying patterns in the text data. It identified and analyzed the most frequent hashtags and user mentions, providing a network visualization of these elements. The provided dataset is a comprehensive collection of Twitter data focused on the summit between US President Biden and Chinese President Xi Jinping in November 2021. It features over 1,000 observations, each representing a Twitter post. These posts include direct tweets, quotes, and retweets related to articles discussing the two leaders. The data also contains user information, such as the names of the individuals tweeting.\n\n\nb. US presidential inaugural speeches\nThis dataset comprises records of US presidential inaugural addresses from President Eisenhower’s era through to President Trump’s tenure. Graphical representations within the dataset illustrate the prevalence of specific terms like “people,” “American,” and “communist” across these speeches.\ni. Any similarities and differences over time and among presidents?\nThe lexical dispersion analysis of U.S. presidential inaugural speeches from Eisenhower to Biden shows that the usage of “American” has become more frequent in recent administrations, suggesting a contemporary emphasis on national identity, while the term “people” consistently figures across speeches, underscoring a constant focus on democratic values.\n\nThe word “American” was seldom used in the past but saw a notable rise starting with Clinton’s administration, indicating a shift in presidential rhetoric toward patriotism. Additionally, the term “communist” saw heightened usage during the Nixon and Reagan eras, aligning with the peak of the Red Scare, reflecting the socio-political concerns of the times.\n4. What is Wordfish? (Do research on quanteda website)\nWordfish is a method used for analyzing texts in a one-dimensional framework, primarily focusing on how frequently words occur within documents. This technique is adept at assessing the positioning of documents on a singular axis, such as an ideological spectrum. By evaluating the usage frequency of specific words, Wordfish helps in organizing and interpreting data, as exemplified in the analysis of inaugural speech data.\n.\nAssigment 4 (Not provided)\nAssignment 5\nYouTube data\n\nRun YouTubenews01.R. (prerequisites: YouTube developer API). Repeat the data collection of CNN’s channel stats, video stats and comments.\nI tried running the code with ID and secret but didn’t work.\nAnalyze the stats and comments\nCan you use quanteda to analyze the text data from YouTube comments?\n\nAssignment 6\nWeb scraping\n\nRun textmining01.R, rvest01.R and rvest02.R.\nOrganize the data in data frames and run text analytics (e.g. Wordcloud)\n\n\n\n\nMLK\n\n\n\n\n\nWC\n\n\n\nChallenge: How to download multiple pdf/data files using webscraping methods?\n\nTo download multiple PDF or data files using web scraping methods, we need to identify the URLs of the files we want to download. This is typically done using a web scraping library like rvest in R to parse the HTML of the webpage and extract the URLs. Once we have the URLs, we can use a function like download.file() in R, in combination with a loop, to programmatically download each file.\nAssignment 7\nGovernment data and parallel processing\n1.       Run govdata01.R and parallel01.R. – Task completed\n2.       Start planning for storage and computational resources: Note the space and time taken.\na.       Plan data management (e.g. database)\nIt took 10.462103 minutes to get 234 records and the total size of PDFs downloaded was 106509630 bytes.\n3.       Organize the data in data frames – Task completed.\n4.       Learn other data storage methods (e.g. arrow, feather, parquet)\nAfter learning about these storage methods, I found that for large, complex datasets, Parquet is often the best choice due to its efficiency in storage and speed in analytics queries. For smaller datasets or when high-speed input output is more critical, Feather is a good choice.\nIf you need to share data between different programming languages or tools, Arrow provides a great platform due to its language-agnostic design.\nAssignment 8\nCensus data and Spatial data\nRead A Guide to Working with US Census Data in R - Task completed\nGet an API key from Census using this website (http://api.census.gov/data/key_signup.html) - Task completed\nRun spatialdata01.R and spatialdata02.R. - Task completed\nThe script “spatialdata01.R” retrieves Census data concerning the median age for each U.S. state in 2019, utilizing the ggplot2 package for visualization. It then generates a thematic map displaying the variations in median age across different states in the U.S.\n\nThe script “spatialdata2” gathers and displays Census data regarding income in Texas, with a special focus on Dallas County for the year 2020. It employs a range of color intensities on the maps to represent various income levels in Texas and Dallas.\n\n\nCompare different years of data (e.g. 2010 and 2020)\nYear 2010\n\n\n\nYear 2020\n\nIn the Dallas area, there’s a noticeable overall uptick in income, particularly in the neighborhoods to the north of downtown Dallas. While incomes were fairly uniform in 2010, especially in the southern regions, more pronounced income disparities have emerged by 2020. The median age reported in the Census has shown little variation, with a marginally reduced median."
  }
]